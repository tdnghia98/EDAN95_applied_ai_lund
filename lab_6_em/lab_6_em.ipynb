{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# EM Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=10)\n",
    "\n",
    "X_train /= 16\n",
    "X_test /= 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1257, 64)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "source": [
    "# Gaussian NB"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNbc():\n",
    "    def __init__(self, seed = 10, epsilon = 1e-3):\n",
    "        self.seed = seed\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def gaussian_prob(self, x, mu, sigma_squared):\n",
    "        return 1/np.sqrt(2*np.pi*sigma_squared) * np.exp(-1/(2*sigma_squared) * (x-mu) ** 2)\n",
    "\n",
    "    def fit(self, X, K):\n",
    "        self.K = K\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        priors = np.ones(K)/K\n",
    "\n",
    "\n",
    "        # Create subsamples of classes ie. creating n_samples of indices of all classes and shuffle.\n",
    "        # The samples are randomly assigned to a class\n",
    "        idxs = np.resize(range(K), n_samples)\n",
    "        np.random.shuffle(idxs)\n",
    "        \n",
    "        mu = np.empty([K, n_features])\n",
    "        sigma_squared = np.empty([K, n_features])\n",
    "\n",
    "        # Calculate the mu and the sqaured sigma (variance) of each class\n",
    "        for k in range(K):\n",
    "            mu[k] = np.mean(X[idxs == k], axis = 0)\n",
    "            sigma_squared[k] = np.var(X[idxs == k], axis = 0)\n",
    "\n",
    "        sigma_squared += self.epsilon\n",
    "        \n",
    "        P = np.empty([n_samples, K])\n",
    "        r = np.empty([n_samples, K])\n",
    "\n",
    "        prev_prior = np.zeros(K)\n",
    "        \n",
    "        while np.linalg.norm(prev_prior - priors) > 1e-6:\n",
    "            # E step\n",
    "            for k in range(K):\n",
    "                P[:, k] = np.prod(self.gaussian_prob(X, mu[k], sigma_squared[k]), axis=1)\n",
    "\n",
    "            r = priors * P / (np.sum(priors * P, axis = 1)).reshape(-1, 1)\n",
    "\n",
    "            # M step\n",
    "            r_k = np.sum(r, axis = 0)\n",
    "            prev_prior = priors\n",
    "\n",
    "            priors = r_k / n_samples\n",
    "\n",
    "            for k in range(K):\n",
    "                mu[k] = np.sum(r[:, k].reshape(-1,1) * X, axis = 0) / r_k[k]\n",
    "                # Our X is a row vector so we need to transpose it and perform xT * x to have a resulting matrix instead of a scalar\n",
    "                sigma_squared[k] = np.diag( ( r[:, k].reshape(-1, 1) * (X - mu[k]) ).T @ (X - mu[k]) / r_k[k] )\n",
    "\n",
    "            sigma_squared += self.epsilon\n",
    "        self.mu = mu\n",
    "        self.sigma_squared = sigma_squared\n",
    "        self.priors = priors\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = np.empty(len(X))\n",
    "        prob = np.zeros(self.K)\n",
    "\n",
    "        for i, x in enumerate(X):\n",
    "            for k in range(self.K):\n",
    "                prob[k] = self.priors[k] * np.prod(self.gaussian_prob(x, self.mu[k], self.sigma_squared[k]))\n",
    "            \n",
    "            preds[i] = np.argmax(prob)\n",
    "\n",
    "        self.preds = preds\n",
    "        return self.preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = GaussianNbc()\n",
    "clustering.fit(X_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clustering.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Completeness score: 0.6821415438378211\nHomogeneity score: 0.6542738047882036\nAMI score: 0.6557012219396601\n"
     ]
    }
   ],
   "source": [
    "print(\"Completeness score: %s\" %(metrics.completeness_score(y_test, predictions)))\n",
    "print(\"Homogeneity score: %s\" %(metrics.homogeneity_score(y_test, predictions)))\n",
    "print(\"AMI score: %s\" %(metrics.adjusted_mutual_info_score(y_test, predictions)))"
   ]
  },
  {
   "source": [
    "# K Means"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansEM():\n",
    "    def __init__(self, seed=10, epsilon=1e-4):\n",
    "        self.seed = seed\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def fit(self, X, K):\n",
    "        self.K = K\n",
    "        np.random.seed(self.seed)\n",
    "        priors = np.ones(K)/K\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "        \n",
    "        idxs = np.resize(range(K), n_samples)\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        # Initialise mu\n",
    "        mu = np.empty([K, n_features])\n",
    "        for k in range(K):\n",
    "            mu[k] = np.mean(X[idxs == k], axis=0)\n",
    "\n",
    "\n",
    "        prev_mu = np.zeros([K, n_features])\n",
    "\n",
    "        while np.sum(np.linalg.norm(mu - prev_mu)) > 1e-6:\n",
    "            # E step\n",
    "            for i, x in enumerate(X):\n",
    "                cluster_distance = np.empty(K)\n",
    "                for k in range(K):\n",
    "                    cluster_distance[k] = np.linalg.norm(x - mu[k])\n",
    "                x_cluster = np.argmin(cluster_distance, axis=0)\n",
    "                idxs[i] = x_cluster\n",
    "\n",
    "            # M step\n",
    "            prev_mu = mu\n",
    "            for k in range(K):\n",
    "                mu[k] = np.mean(X[idxs == k], axis=0)\n",
    "        \n",
    "        self.mu = mu\n",
    "    \n",
    "    def predict(self, X):\n",
    "        preds = np.empty(len(X))\n",
    "        cluster_distance = np.zeros(self.K)\n",
    "\n",
    "        for i, x in enumerate(X):\n",
    "            for k in range(self.K):\n",
    "                cluster_distance[k] = np.linalg.norm(x - self.mu[k])\n",
    "            preds[i] = np.argmin(cluster_distance, axis=0)\n",
    "\n",
    "        self.preds = preds\n",
    "        return self.preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeansEM()\n",
    "kmeans.fit(X_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Homogeneity score: 0.5451246898456436\nCompleteness score: 0.5615544031918106\nAMI score: 0.5369016946840789\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity score: %s\" %metrics.homogeneity_score(y_test, preds))\n",
    "print(\"Completeness score: %s\" %metrics.completeness_score(y_test, preds))\n",
    "print(\"AMI score: %s\" %metrics.adjusted_mutual_info_score(y_test, preds))"
   ]
  },
  {
   "source": [
    "# K Means Scikit"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "sk_kmeans = KMeans(10)\n",
    "sk_kmeans.fit(X_train)\n",
    "preds = sk_kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Homogeneity score: 0.739391403680697\nCompleteness score: 0.7540718109012448\nAMI score: 0.7375062058724509\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity score: %s\" %metrics.homogeneity_score(y_test, preds))\n",
    "print(\"Completeness score: %s\" %metrics.completeness_score(y_test, preds))\n",
    "print(\"AMI score: %s\" %metrics.adjusted_mutual_info_score(y_test, preds))"
   ]
  }
 ]
}